export const portfolioProjects = [
  {
    id: 'careerxr',
    title: 'CareerXR',
    description: 'CareerXR is an innovative VR application designed to transform the way people prepare for job interviews. Leveraging cutting-edge virtual reality and AI-driven interaction, CareerXR immerses users in a fully simulated interview environment, recreating the high-stakes feel of real-life interviews.',
    fullDescription: `CareerXR is an innovative VR application designed to transform the way people prepare for job interviews. Leveraging cutting-edge virtual reality and AI-driven interaction, CareerXR immerses users in a fully simulated interview environment, recreating the high-stakes feel of real-life interviews. The application dynamically adjusts to user responses, providing tailored follow-up questions, feedback, and suggestions to enhance interview skills over time.

Within CareerXR, users can select from various interview scenarios and industries, practice for behavioral and technical questions, and even customize the difficulty level to meet their specific needs. The AI behind CareerXR evaluates body language, tone, and content of responses, offering personalized insights to help users refine their delivery and confidence.

Perfect for job seekers, career switchers, and students, CareerXR provides a unique blend of realism and adaptability, ensuring that users are fully prepared to tackle interviews with poise and professionalism.`,
    image: '/images/careerxr.png',
    categories: ['VR', 'Productivity', 'Learning'],
    role: ['Lead VR Developer'],
    videoUrl: 'https://www.youtube.com/embed/rznnxzi8mK0',
    websiteUrl: null,
  },
  {
    id: 'obnd',
    title: 'Old Books & Dice',
    description: 'Reimagine the tabletop role-playing experience across platforms, empowering brands, creators, and players to craft and immerse in epic stories together, all powered by AI.',
    fullDescription: 'Reimagine the tabletop role-playing experience across platforms, empowering brands, creators, and players to craft and immerse in epic stories together, all powered by AI.',
    image: '/images/obnd.png',
    categories: ['VR', 'RPG'],
    role: ['Lead VR Developer'],
    videoUrl: null,
    videoUrls: ['https://www.youtube.com/embed/EW-nBn2Tetc', 'https://www.youtube.com/embed/zGW4Z8UhhQ8'],
    websiteUrl: null,
    gallery: null,
  },
  {
    id: 'yur',
    title: 'YUR World',
    description: 'YUR World® is your virtual world. Ignite your imagination and heart rate. Explore new worlds, compete, and map your progress. With YUR\'s community by your side, you\'ll never go it alone.',
    fullDescription: 'YUR World® is your virtual world. Ignite your imagination and heart rate. Explore new worlds, compete, and map your progress. With YUR\'s community by your side, you\'ll never go it alone. Enjoy the journey, not just the destination.',
    image: '/images/yurworld.png',
    categories: ['XR', 'Fitness'],
    role: ['Gameplay Engineer'],
    videoUrl: null,
    websiteUrl: 'https://yur.world/',
    gallery: null,
  },
  {
    id: 'oww',
    title: 'Occupy White Walls',
    description: 'Enter Occupy White Walls, a PC sandbox-building, AI-driven MMO game about building and curating your own art gallery, while allowing players to discover both classic artworks from historical figures, to contemporary paintings from modern artists.',
    fullDescription: 'Enter Occupy White Walls, a PC sandbox-building, AI-driven MMO game about building and curating your own art gallery, while allowing players to discover both classic artworks from historical figures, to contemporary paintings from modern artists.',
    image: '/images/oww.png',
    categories: ['MMO', 'Sandbox', 'AI-driven'],
    role: ['Senior Unreal Engine Programmer'],
    videoUrl: null,
    websiteUrl: 'https://oww.io/',
    gallery: ['/images/oww/oww1.png', '/images/oww/oww2.png', '/images/oww/oww3.png', '/images/oww/oww4.png', '/images/oww/oww5.png'],
  },
  {
    id: 'btc',
    title: 'Between Two Castles',
    description: 'Between Two Castles of Mad King Ludwig is a competitive tile-drafting game in which each tile is a room in a castle. You work together with two other players to design two different castles.',
    fullDescription: `Between Two Castles of Mad King Ludwig is a competitive tile-drafting game in which each tile is a room in a castle. You work together with two other players to design two different castles. On each turn you select two tiles from your hand, reveal them, then work with your partners to place them. To win, you have to share your attention and your devotion between two castles.`,
    image: '/images/b2c.jpg',
    categories: ['Competitive', 'Tile-drafting'],
    role: ['Gameplay Programmer'],
    videoUrl: 'https://www.youtube.com/embed/V_fQw59pUWM',
    websiteUrl: 'https://store.steampowered.com/app/1158500/Between_Two_Castles__Digital_Edition/',
    gallery: ['/images/btc/btc1.png', '/images/btc/btc2.png', '/images/btc/btc3.png'],
  },
  {
    id: 'infamy',
    title: 'Infamy',
    description: 'In the Martian mining colony of ARES-6, crime pays. Three factions vie for control of this corrupt new world and everything within it.',
    fullDescription: `In the Martian mining colony of ARES-6, crime pays. Three factions vie for control of this corrupt new world and everything within it. You are a mercenary known as a "freelancer", here to profit off the conflict, to make a name for yourself – but ambition alone isn't enough. A network of seedy contacts will assist you in undertaking the dangerous missions necessary to bolster your rep. Whether it's hiring henchmen to carry out your dirty work or plotting with secret schemes, you'll let nothing stand between you and your squalid goals.`,
    image: '/images/infamy.jpg',
    categories: ['Strategy', 'Cyberpunk'],
    role: ['Gameplay Programmer'],
    videoUrl: 'https://www.youtube.com/embed/qds98BSwXKg',
    websiteUrl: null,
    gallery: ['/images/infamy/infamy1.png', '/images/infamy/infamy2.png', '/images/infamy/infamy3.png', '/images/infamy/infamy4.png', '/images/infamy/infamy5.png', '/images/infamy/infamy6.gif'],
  },
  {
    id: 'unearth',
    title: 'Unearth',
    description: 'Long ago, your ancestors built great cities across the world. Now your tribe must explore forests, deserts, islands, mountains, and caverns to find these lost cities.',
    fullDescription: `Long ago, your ancestors built great cities across the world. Now your tribe must explore forests, deserts, islands, mountains, and caverns to find these lost cities. Claim the ruins, build places of power, and restore the glory of a bygone age. Unearth is a mobile bend-your-luck game of dice placement and set collection. Designed by Jason Harner and Matthew Ransom, it plays in under an hour with 2-4 players. Each player leads a tribe of Delvers, represented by five dice (3 six-sided, 1 four-sided, and 1 eight-sided). Players take turns rolling and placing dice in an attempt to claim Ruins.`,
    image: '/images/unearth.jpg',
    categories: ['Bend-your-luck', 'Dice placement', 'Set collection'],
    role: ['Gameplay Programmer'],
    videoUrl: 'https://www.youtube.com/embed/NjexD-4zN6g',
    websiteUrl: null,
    gallery: ['/images/unearth/unearth1.png', '/images/unearth/unearth2.png', '/images/unearth/unearth3.png', '/images/unearth/unearth4.png'],
  },
  {
    id: 'data',
    title: 'Data Projects',
    description: 'Documenting projects I\'ve been working on related to Data Engineering',
    fullDescription: 'Documenting projects I\'ve been working on related to Data Engineering',
    image: '/images/dataprojects.png',
    categories: ['Data Engineering', 'ETL', 'Analytics'],
    role: ['Data Engineer'],
    videoUrl: null,
    websiteUrl: null,
    isDataProjects: true,
    projects: [
      {
        title: 'Automated Stock Market Data Pipeline with Astronomer, Airflow, Python and Snowflake',
        description: 'Designed and deployed a fully automated data pipeline using Apache Airflow managed by Astronomer to ingest and store stock market data. The pipeline retrieves structured data from a public REST API, processes it using Pandas, and stores the results in Snowflake for downstream analytics and reporting.',
        details: [
          'Used the @dag and @task decorators from Airflow\'s functional API to define a DAG scheduled to run every 12h (0 */12 * * *), ensuring up-to-date data ingestion.',
          'Fetched real-time data on top stock gainers by sending GET requests to a configurable REST endpoint. The response is normalized and transformed into a pandas.DataFrame.',
          'Utilized the snowflake-connector python library to connect to a Snowflake data warehouse.',
        ],
        images: [
          '/images/data/astronomer.png',
          '/images/data/stock_pipeline_dag.png',
          '/images/data/stock_pipeline_tasks.png',
        ],
        code: {
          language: 'python',
          content: `from airflow.decorators import dag, task
from datetime import datetime
import pandas as pd
import requests
import snowflake.connector
import os
from dotenv import load_dotenv

load_dotenv()

# Configuration from .env
REST_API_URL = os.getenv("REST_API_URL")
SNOWFLAKE_CONFIG = {
    "user": os.getenv("SNOWFLAKE_USER"),
    "password": os.getenv("SNOWFLAKE_PASSWORD"),
    "account": os.getenv("SNOWFLAKE_ACCOUNT"),
    "warehouse": os.getenv("SNOWFLAKE_WAREHOUSE"),
    "database": os.getenv("SNOWFLAKE_DATABASE"),
    "schema": os.getenv("SNOWFLAKE_SCHEMA"),
}
TABLE_NAME = "TOP_GAINERS"

@task
def fetch_json_from_api(url):
    response = requests.get(url)
    response.raise_for_status()
    data = response.json()
    df = pd.json_normalize(data)
    print(f"Loaded {len(df)} records from API.")
    return df

@task
def upload_to_snowflake(df, config, table_name):
    ctx = snowflake.connector.connect(autocommit=True, **config)
    cs = ctx.cursor()

    try:
        columns = ", ".join(f'"{col}" STRING' for col in df.columns)
        cs.execute(f"""
            CREATE TABLE IF NOT EXISTS {table_name} (
                {columns}
            );
        """)

        for _, row in df.iterrows():
            values = ", ".join(f"'{str(v).replace("'", "''")}'" for v in row)
            cs.execute(f"INSERT INTO {table_name} VALUES ({values});")

        print("Upload complete.")
    finally:
        cs.close()
        ctx.close()

@dag(
    dag_id='stock_gainers_pipeline',
    start_date=datetime(2025, 5, 12),
    schedule='0 */12 * * *',
    catchup=False,
    default_args={
        'owner': 'airflow',
        'retries': 1,
    },
)
def stock_gainers_dag():
    df = fetch_json_from_api(REST_API_URL)
    upload_to_snowflake(df, SNOWFLAKE_CONFIG, TABLE_NAME)

stock_gainers_dag()`,
        },
      },
      {
        title: 'Crypto Data ETL Scraper',
        description: 'This project is a Python-based web scraper that collects real-time data of the top 100 cryptocurrencies from CoinGecko and stores it in both a local CSV file and Google Sheets. Using Selenium for web scraping, the script extracts details like coin names, symbols, and prices, handling dynamic web content and cookie popups. The data is processed with pandas and exported via the gspread API for easy access and sharing. Features include headless browser operation, error logging, and a clean context manager for driver setup.',
        details: [],
        images: [
          '/images/data/PythonETL.png',
          '/images/data/WebScrapingToSpreadsheet.png',
        ],
        code: {
          language: 'python',
          content: `from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import pandas as pd
import gspread
from oauth2client.service_account import ServiceAccountCredentials
from dotenv import load_dotenv
import os
from contextlib import contextmanager
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@contextmanager
def setup_driver():
    """Context manager for Chrome driver setup and cleanup"""
    service = Service(os.path.join(os.getcwd(), "chromedriver.exe"))
    options = webdriver.ChromeOptions()
    options.add_argument('--headless')
    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36')
    driver = webdriver.Chrome(service=service, options=options)
    try:
        yield driver
    finally:
        driver.quit()

def scrape_coingecko(driver):
    """Scrape cryptocurrency data from CoinGecko"""
    try:
        driver.get("https://www.coingecko.com/")
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight / 2);")
        
        WebDriverWait(driver, 20).until(
            EC.presence_of_element_located((By.XPATH, '//table//tr[contains(., "Bitcoin")]'))
        )

        # Handle cookie popup
        try:
            WebDriverWait(driver, 5).until(
                EC.element_to_be_clickable((By.XPATH, '//button[contains(text(), "Accept")]'))
            ).click()
        except:
            logger.info("No cookie popup found")

        # Extract data
        rows = driver.find_elements(By.CSS_SELECTOR, 'table tbody tr')
        data = []
        
        for i, row in enumerate(rows, 1):
            try:
                # Extract name (only the direct text of div.tw-font-semibold)
                name_element = row.find_element(By.CSS_SELECTOR, 'td:nth-child(3) div.tw-font-semibold')
                name = driver.execute_script(
                    "return arguments[0].childNodes[0].nodeValue.trim()", name_element
                )
                symbol = row.find_element(By.CSS_SELECTOR, 'td:nth-child(3) div.tw-text-xs').text.strip()
                price = row.find_element(By.CSS_SELECTOR, 'td:nth-child(5) span[data-price-target="price"]').text.strip()
                price_clean = price.replace('$', '').replace(',', '')
                data.append({'Name': name, 'Symbol': symbol, 'Price': price_clean})
                logger.info(f"Row {i}: {name}, {symbol}, {price}")
            except Exception as e:
                logger.error(f"Error processing row {i}: {e}")

        return data

    except Exception as e:
        logger.error(f"Scraping error: {e}")
        with open('page_source.html', 'w', encoding='utf-8') as f:
            f.write(driver.page_source)
        logger.info("Page source saved to 'page_source.html'")
        return []

def save_to_csv(data):
    """Save data to CSV file"""
    df = pd.DataFrame(data)
    if not df.empty:
        df.to_csv('coingecko_top_coins.csv', index=False)
        logger.info("\\nDataFrame head:\\n%s", df.head())
    else:
        logger.warning("No data to save to CSV")
    return df

def update_google_sheets(df):
    """Update Google Sheets with data"""
    load_dotenv()
    SPREADSHEET_ID = os.getenv("SPREADSHEET_ID")
    
    scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
    creds = ServiceAccountCredentials.from_json_keyfile_name("google_credentials.json", scope)
    client = gspread.authorize(creds)
    
    spreadsheet = client.open_by_key(SPREADSHEET_ID)
    worksheet = spreadsheet.worksheet("Test")
    
    worksheet.update(
        values=[df.columns.values.tolist()] + df.values.tolist(),
        range_name='A1'
    )
    logger.info("Data successfully written to Google Sheets")

def main():
    """Main execution function"""
    with setup_driver() as driver:
        data = scrape_coingecko(driver)
        df = save_to_csv(data)
        if not df.empty:
            update_google_sheets(df)

if __name__ == "__main__":
    main()`,
        },
      },
      {
        title: 'ETL using Node.js',
        description: 'In this ETL process, I used Node.js along with the xml-flow and fast-csv libraries to stream and transform XML data into a CSV file using a predefined delimiter. By processing each <item> element as it is parsed, the code avoids loading the entire XML file into memory, making it suitable for large datasets. The implementation is fully non-blocking, leveraging Node.js\'s asynchronous stream-based architecture to efficiently handle I/O operations without blocking the event loop. All files are processed locally in this example.',
        details: [],
        images: [
          '/images/data/NodejsETL.png',
        ],
        code: {
          language: 'javascript',
          content: `const fs = require("fs");
const xmlFlow = require("xml-flow");
const { format } = require("fast-csv");

const DELIMITER = "," // Change to "\\t" for tab-separated values
const xmlFilePath = "data.xml";
const csvFilePath = "output.csv";

// Create readable stream for XML file
const readStream = fs.createReadStream(xmlFilePath, { encoding: "utf8" });
const writeStream = fs.createWriteStream(csvFilePath);

// Initialize XML streaming parser
const xmlStream = xmlFlow(readStream);

// Create CSV stream
const csvStream = format({ headers: true, delimiter: DELIMITER });
csvStream.pipe(writeStream);

// Process each item element in XML
xmlStream.on("tag:item", item => {
    csvStream.write({
        id: item.id,
        name: item.name,
        value: item.value
    });
});

xmlStream.on("end", () => {
    csvStream.end();
    console.log("CSV file written successfully.");
});`,
        },
        additionalCode: {
          description: 'This code uses mysql2 and fast-csv libraries to connect to a MySQL database, retrieves all rows from the items table, and streams them to a CSV file. It\'s asynchronous, memory-efficient, and handles both database and file I/O without blocking the Node.js event loop.',
          language: 'javascript',
          content: `const mysql = require("mysql2/promise");
const fs = require("fs");
const { writeToStream } = require("fast-csv");

const DELIMITER = ",";

async function sqlToCsv(csvFilePath, delimiter = DELIMITER) {
    const connection = await mysql.createConnection({
        host: "localhost",
        user: "user",
        password: "password",
        database: "database"
    });

    try {
        console.log("Connected to MySQL database.");

        const [rows] = await connection.execute("SELECT * FROM items");

        await new Promise((resolve, reject) => {
            const ws = fs.createWriteStream(csvFilePath);
            writeToStream(ws, rows, { headers: true, delimiter })
                .on("finish", resolve)
                .on("error", reject);
        });

        console.log("CSV file written successfully!");
    } catch (err) {
        console.error("Error:", err.message);
    } finally {
        await connection.end();
        console.log("Connection closed.");
    }
}

sqlToCsv("output.csv");`,
        },
      },
      {
        title: 'Serverless Event Driven Data Processing',
        description: 'This solution is for situations where files are uploaded on an ad hoc basis and require transformation before being loaded into a data target. In this case, files are uploaded in JSON format and converted to Parquet. Thanks to the SNS->SQS pipeline, the system is highly scalable, even with AWS Lambda limitations, and it\'s also extendable if there\'s a need to use the same event from the file upload in the data source for other purposes.',
        details: [],
        images: [
          '/images/data/ServerlessEventDrivenDataProcessing.svg',
        ],
        code: {
          language: 'python',
          content: `import boto3
import pandas as pd
from io import StringIO, BytesIO
import json

s3 = boto3.client('s3')

def lambda_handler(event, context):
    try:
        print("Starting lambda execution")
        
        # Loop through SQS messages
        for record in event['Records']:
            print("Processing record")
            
            # Extract the body of the SQS message
            sqs_body = record['body']
            sns_message = json.loads(sqs_body)['Message']
            s3_event = json.loads(sns_message)
            
            bucket_name = s3_event['detail']['bucket']['name']
            object_key = s3_event['detail']['object']['key']
            
            print(f"Fetching object from S3: Bucket={bucket_name}, Key={object_key}")
            s3_object = s3.get_object(Bucket=bucket_name, Key=object_key)
            
            object_content = s3_object['Body'].read().decode('utf-8')
            print("Fetched S3 object content")
            
            json_data = json.loads(object_content)
            df = pd.json_normalize(json_data)
            print("Converted JSON to DataFrame")
            
            # Convert DataFrame to Parquet and write to an in-memory buffer
            buffer = BytesIO()
            df.to_parquet(buffer, engine='pyarrow')
            buffer.seek(0)  # Reset buffer pointer
            print("Converted DataFrame to Parquet")
            
            # Upload the parquet file to S3
            s3.upload_fileobj(buffer, 'endpoint-data-driven-processing-test', object_key.replace('json', 'parquet'))
            print("Uploaded Parquet file to S3")
        
        return {
            'statusCode': 200,
            'body': json.dumps('Successfully processed S3 event')
        }
    
    except Exception as e:
        print(f"Error processing event: {str(e)}")
        return {
            'statusCode': 500,
            'body': json.dumps(f"Error: {str(e)}")
        }`,
        },
      },
    ],
  },
];

export const personalInfo = {
  name: 'Athos Santos',
  title: 'Software Engineer & Game Developer',
  bio: `Started learning game development as a teenager working on some old VB6 engines and RPG Maker, then decided to move into general programming, learning many languages such as C#, Python and Lua, but sticked to C and C++.

For years now I've been working professionally on multiple titles for many platforms such as PC, Android and Meta Quest, mostly using Unreal Engine.`,
  image: '/images/me.jpg',
  resume: '/assets/Athos Santos - Resume.pdf',
  social: {
    linkedin: 'https://www.linkedin.com/in/athos-santos/',
    github: 'https://github.com/athosr',
    facebook: 'https://www.facebook.com/athosrls',
  },
};

